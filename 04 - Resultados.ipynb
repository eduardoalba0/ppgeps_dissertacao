{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Importações"
   ],
   "id": "c16c8a44eca4456b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T00:50:33.520636Z",
     "start_time": "2025-02-11T00:50:30.253174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas\n",
    "import cudf as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import shap\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.src.layers import Input, LSTM, Dense\n",
    "from pyESN import ESN\n",
    "from shap.plots import colors\n",
    "from sklearn.ensemble import RandomForestRegressor as RandomForest\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "SEED = 100\n",
    "\n",
    "\n",
    "def reset_seed(rnd_seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "    random.seed(rnd_seed)\n",
    "    np.random.seed(rnd_seed)\n",
    "    tf.random.set_seed(rnd_seed)\n",
    "\n",
    "\n",
    "def rrmse(actual, predicted) -> int:\n",
    "    return root_mean_squared_error(actual, predicted) / np.mean(actual)\n",
    "\n",
    "\n",
    "def smape(actual, predicted) -> int:\n",
    "    if not all([isinstance(actual, np.ndarray),\n",
    "                isinstance(predicted, np.ndarray)]):\n",
    "        actual, predicted = np.array(actual), np.array(predicted)\n",
    "\n",
    "    return round(\n",
    "        np.mean(\n",
    "            np.abs(predicted - actual) /\n",
    "            ((np.abs(predicted) + np.abs(actual)) / 2)\n",
    "        ) * 100, 2\n",
    "    )\n",
    "\n",
    "\n",
    "reset_seed()\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/path/to/cuda'"
   ],
   "id": "e704049bd3c651bc",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Carregar Datasets",
   "id": "b2c3ca5b81a8a230"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T00:50:40.466768Z",
     "start_time": "2025-02-11T00:50:34.920386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_mesclado = pd.read_csv(\"./dados/dados_mesclados.csv\", sep=';', decimal='.')\n",
    "df_features = pandas.read_csv(\"./dados/fitness_features_agrupado.csv\", sep=\";\", decimal=\".\")\n",
    "\n",
    "df_features = df_features.sort_values(\"MAPE\").head(1).reset_index(drop=True)\n",
    "df_features = pandas.DataFrame(\n",
    "    columns=str(df_features.iloc[0][\"FEATURES\"]).replace(\"(\", '').replace(\")\", '').replace(\"'\", \"\").split(\", \"))\n",
    "\n"
   ],
   "id": "50df6794075724cd",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Melhores Parâmetros",
   "id": "6a5d9165742b4919"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T00:48:13.927095Z",
     "start_time": "2025-02-11T00:48:13.825881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best = {}\n",
    "for modelo in [\"ESN\", \"LSTM\", \"RF\", \"XGB\"]:\n",
    "    df_aux = pd.DataFrame()\n",
    "    for optimizer in [\"GA\", \"PSO\"]:\n",
    "        for seed in [\"10000\", \"20000\", \"30000\"]:\n",
    "            new_df = pd.read_csv(f'parâmetros/{optimizer}-{modelo} ITERS SEED {seed}.csv', sep=\";\", decimal=\".\",\n",
    "                                 header=0)\n",
    "            df_aux = pd.concat([df_aux, new_df], axis=0)\n",
    "\n",
    "    df_aux = df_aux.sort_values(by=[\"Fitness\"])\n",
    "    df_aux[df_aux.isnull()] = None\n",
    "    best[f\"{modelo}\"] = df_aux[:1].iloc[0].to_dict()"
   ],
   "id": "1a1f80225e7ae9a6",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Criação dos Modelos",
   "id": "9ed9a2ba5c763ded"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T00:50:44.648793Z",
     "start_time": "2025-02-11T00:50:42.569502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#ESN\n",
    "esn = ESN(n_inputs=df_mesclado[df_features].shape[1],\n",
    "          n_outputs=1,\n",
    "          n_reservoir=int(best[\"ESN\"][\"Reservoirs\"]),\n",
    "          sparsity=best[\"ESN\"][\"Sparsity\"],\n",
    "          spectral_radius=best[\"ESN\"][\"Spectral Radius\"],\n",
    "          noise=best[\"ESN\"][\"Noise\"],\n",
    "          random_state=SEED)\n",
    "\n",
    "#LSTM\n",
    "tf.keras.backend.clear_session()\n",
    "lstm = Sequential([\n",
    "    Input((df_mesclado[df_features].shape[1], 1)),\n",
    "    LSTM(best[\"LSTM\"][\"Units\"],\n",
    "         activation=best[\"LSTM\"][\"Activation\"],\n",
    "         use_bias=best[\"LSTM\"][\"Bias\"],\n",
    "         seed=SEED),\n",
    "    Dense(1),\n",
    "])\n",
    "lstm.compile(loss='mape')\n",
    "\n",
    "#RF\n",
    "rf = RandomForest(random_state=SEED,\n",
    "                  n_estimators=int(best[\"RF\"][\"N_estimators\"]),\n",
    "                  max_depth=int(best[\"RF\"][\"Max_depth\"]),\n",
    "                  min_samples_split=int(best[\"RF\"][\"Min_samples_split\"]),\n",
    "                  min_samples_leaf=int(best[\"RF\"][\"Min_samples_leaf\"]))\n",
    "\n",
    "#XGB\n",
    "updater = \"coord_descent\" if best[\"XGB\"][\"Booster\"] == \"gblinear\" else None\n",
    "xgb = XGBRegressor(random_state=SEED,\n",
    "                   n_estimators=int(best[\"XGB\"][\"N_estimators\"]),\n",
    "                   max_depth=int(best[\"XGB\"][\"Max_depth\"]),\n",
    "                   booster=best[\"XGB\"][\"Booster\"],\n",
    "                   reg_lambda=best[\"XGB\"][\"Lambda\"],\n",
    "                   reg_alpha=best[\"XGB\"][\"Alpha\"],\n",
    "                   updater=updater)\n"
   ],
   "id": "7f51b4f7d9a2737f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1739235042.913290    5203 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1739235043.184368    5203 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1739235043.184647    5203 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1739235043.193707    5203 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1739235043.193935    5203 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1739235043.194010    5203 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1739235043.195159    5203 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1739235043.195461    5203 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-10 21:50:43.195512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1739235043.195619    5203 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-10 21:50:43.195647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4041 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Treino e Teste\n",
    "## Divisão dos Dados"
   ],
   "id": "9f7829887c14e9c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dataframes_treino = []\n",
    "dataframes_teste = []\n",
    "\n",
    "for campus, dados in df_mesclado.groupby(\"CAMPUS\"):\n",
    "    dados[\"CAMPUS\"] = campus\n",
    "    for i in range(1, 12 + 1):\n",
    "        lag = dados['CONSUMO'].shift(i)\n",
    "        dados[f'LAG_' + '{:02d}'.format(i)] = lag\n",
    "    dados.dropna(inplace=True)\n",
    "\n",
    "    treino, teste = train_test_split(dados, test_size=12, random_state=SEED, shuffle=False)\n",
    "\n",
    "    dataframes_treino.append(treino)\n",
    "    dataframes_teste.append(teste)\n",
    "\n",
    "df_features = df_mesclado.drop(df_mesclado.drop(df_features, axis=1).columns.to_list(), axis=1).columns\n",
    "df_treino = pd.concat(dataframes_treino, ignore_index=True).sort_values(\"CAMPUS\").sort_values(\"DATA\")\n",
    "df_teste = pd.concat(dataframes_teste, ignore_index=True).sort_values(\"CAMPUS\").sort_values(\"DATA\")\n"
   ],
   "id": "c13d6c7a5c9c1df8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Regressão\n",
   "id": "94d525f158ae8452"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T01:11:35.424504Z",
     "start_time": "2025-02-11T01:11:35.417598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def treino_kfold(modelo, dataframe, features):\n",
    "    cvs = []\n",
    "    subdf = {data: dados for data, dados in dataframe.sort_values(\"CAMPUS\", ascending=True).groupby('DATA')}\n",
    "    for i_treino, i_teste in TimeSeriesSplit(n_splits=5, test_size=1).split(subdf):\n",
    "        i_treino = [list(subdf.keys())[index] for index in i_treino]\n",
    "        i_teste = [list(subdf.keys())[index] for index in i_teste]\n",
    "\n",
    "        dados_treino = pd.concat([subdf[index] for index in i_treino], ignore_index=True)\n",
    "        dados_teste = pd.concat([subdf[index] for index in i_teste], ignore_index=True)\n",
    "\n",
    "        x_treino, y_treino = dados_treino[features].astype(np.float32).to_cupy().get(), dados_treino[\n",
    "            \"CONSUMO\"].astype(\n",
    "            np.float32).to_cupy().get()\n",
    "        x_teste, y_teste = dados_teste[features].astype(np.float32).to_cupy().get(), dados_teste[\n",
    "            \"CONSUMO\"].to_cupy().get()\n",
    "\n",
    "        y_previsto = []\n",
    "        if isinstance(modelo, RandomForest) or isinstance(modelo, XGBRegressor):\n",
    "            modelo.fit(x_treino, y_treino)\n",
    "            for row in x_teste:\n",
    "                previsao = modelo.predict(row.reshape(1, -1))\n",
    "                y_previsto.append(previsao)\n",
    "\n",
    "        else:\n",
    "            if isinstance(modelo, ESN):\n",
    "                modelo.fit(x_treino, y_treino)\n",
    "            else:\n",
    "                modelo.fit(x_treino, y_treino, shuffle=False, verbose=False, epochs=best[\"LSTM\"][\"Epochs\"],\n",
    "                           batch_size=best[\"LSTM\"][\"Batch Size\"])\n",
    "            y_previsto = []\n",
    "            for row in x_teste:\n",
    "                previsao = modelo.predict(row.reshape(1, -1))[0]\n",
    "                y_previsto.append(previsao)\n",
    "\n",
    "        mape = mean_absolute_percentage_error(y_teste, y_previsto)\n",
    "        cvs.append(mape)\n",
    "\n",
    "    return modelo, cvs\n",
    "\n",
    "\n",
    "def teste_split(modelo, dataframe_treino, dataframe_teste, features, horizonte):\n",
    "    dataframe_treino = dataframe_treino.sort_values(\"CAMPUS\", ascending=True).sort_values(\"DATA\")\n",
    "    dataframe_teste = dataframe_teste.sort_values(\"CAMPUS\", ascending=True).sort_values(\"DATA\")\n",
    "\n",
    "    x_treino = dataframe_treino[features].astype(np.float32).to_cupy().get()\n",
    "    y_treino = dataframe_treino[\"CONSUMO\"].astype(np.float32).to_cupy().get()\n",
    "    x_teste = dataframe_teste[features].astype(np.float32)\n",
    "    y_teste = dataframe_teste[\"CONSUMO\"].astype(np.float32).to_cupy().get()\n",
    "\n",
    "    preds = []\n",
    "    datasets = dataframe_treino.copy()\n",
    "\n",
    "    if isinstance(modelo, RandomForest) or isinstance(modelo, XGBRegressor):\n",
    "        modelo.fit(x_treino, y_treino)\n",
    "    else:\n",
    "        if isinstance(modelo, ESN):\n",
    "            modelo.fit(x_treino, y_treino)\n",
    "        else:\n",
    "            modelo.fit(x_treino, y_treino, shuffle=False, verbose=False, epochs=best[\"LSTM\"][\"Epochs\"],\n",
    "                       batch_size=best[\"LSTM\"][\"Batch Size\"])\n",
    "\n",
    "    for i_test in range(horizonte):\n",
    "        sx_test = x_teste.iloc[[i_test]]\n",
    "        sx_test_aux = sx_test.copy()\n",
    "\n",
    "        datasets = pd.concat([datasets, sx_test_aux], axis=0)\n",
    "        for lag in range(1, 12 + 1):\n",
    "            if 'LAG_' + \"{:02d}\".format(lag) in sx_test.columns:\n",
    "                sx_test_aux[f'LAG_' + '{:02d}'.format(lag)] = datasets[\"CONSUMO\"].shift(lag)\n",
    "                datasets[f'LAG_' + '{:02d}'.format(lag)] = datasets[\"CONSUMO\"].shift(lag)\n",
    "\n",
    "        if isinstance(modelo, RandomForest) or isinstance(modelo, XGBRegressor):\n",
    "            pred = modelo.predict(sx_test_aux.to_cupy().get().reshape(1, -1))\n",
    "        else:\n",
    "            pred = modelo.predict(sx_test_aux.to_cupy().get().reshape(1, -1))[0]\n",
    "\n",
    "        sx_test_aux[\"consumption\"] = pred\n",
    "        preds.append(pred)\n",
    "        datasets.update(sx_test_aux)\n",
    "\n",
    "    medidas = pd.Series([rrmse(y_teste, preds), smape(y_teste, preds)], index=[\"RRMSE\", \"sMAPE\"])\n",
    "\n",
    "    return medidas, preds, datasets\n"
   ],
   "id": "c671c20d67739a3f",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Treino com todos os campi",
   "id": "69d895aa843ef3f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a4811c2faa852c81"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Treino com campi individuais",
   "id": "5ba822cd77a3efa6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e02804aee519d91b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
