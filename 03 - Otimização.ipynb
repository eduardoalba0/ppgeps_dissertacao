{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importações",
   "id": "b8211b6fd209c236"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T20:03:49.011128Z",
     "start_time": "2025-01-25T20:03:48.990277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cudf as pd\n",
    "import cupy\n",
    "import dask\n",
    "import gc\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from pyESN import ESN\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from cuml import RandomForestRegressor as CudaRandomForest\n",
    "from keras import Sequential\n",
    "from keras.src.layers import Input, LSTM, Dense\n",
    "from pyswarms.single import GlobalBestPSO\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "\n",
    "SEED = 100\n",
    "\n",
    "\n",
    "def reset_seed(rnd_seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "    random.seed(rnd_seed)\n",
    "    np.random.seed(rnd_seed)\n",
    "    cupy.random.seed(rnd_seed)\n",
    "    tf.random.set_seed(rnd_seed)\n",
    "\n",
    "\n",
    "reset_seed()\n",
    "dask.config.set(scheduler=\"threads\", num_workers=10)\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/path/to/cuda'\n"
   ],
   "id": "13ca1a58c23b6ba6",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Configuração dos Otimizadores\n",
    "## Algoritmo Genético (GA)"
   ],
   "id": "b0cc5812b8975644"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ESN",
   "id": "778bbf996e24f682"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T20:03:49.112905Z",
     "start_time": "2025-01-25T20:03:49.100307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IndESN:\n",
    "    def __init__(self):\n",
    "        self.fitness = None\n",
    "        self.n_reservoirs = 0\n",
    "        self.sparsity = 0\n",
    "        self.spectral_radius = 0\n",
    "        self.noise = 0\n",
    "\n",
    "    def create_random(self):\n",
    "        self.rand_n_reservoirs()\n",
    "        self.rand_sparsity()\n",
    "        self.rand_spectral_radius()\n",
    "        self.rand_noise()\n",
    "        return self\n",
    "\n",
    "    def rand_n_reservoirs(self):\n",
    "        self.n_reservoirs = random.randint(2, 1000)\n",
    "\n",
    "    def rand_sparsity(self):\n",
    "        self.sparsity = random.uniform(0.1, 0.5)\n",
    "\n",
    "    def rand_spectral_radius(self):\n",
    "        self.spectral_radius = random.uniform(0.1, 1.9)\n",
    "\n",
    "    def rand_noise(self):\n",
    "        self.noise = random.uniform(0.0001, 0.8)\n",
    "\n",
    "\n",
    "class GAESN:\n",
    "    def __init__(self, dataset, features, n_individuals, n_generations, seed=SEED):\n",
    "        reset_seed(seed)\n",
    "        self.seed = seed\n",
    "        self.dataset = dataset\n",
    "        self.features = features\n",
    "        self.n_individuals = n_individuals\n",
    "        self.n_generations = n_generations\n",
    "        self.mutation_rate = 0.5\n",
    "        self.population = {}\n",
    "        self.iters = []\n",
    "        self.init_pop()\n",
    "        self.init_gen()\n",
    "\n",
    "    def init_pop(self):\n",
    "        futures = [dask.delayed(self.create_ind)(_) for _ in range(self.n_individuals)]\n",
    "        self.population = dask.compute(futures)[0]\n",
    "        self.population = sorted(self.population, key=lambda a: a.fitness)\n",
    "        best = self.population[0]\n",
    "        self.iters.append(best)\n",
    "\n",
    "    def create_ind(self, i):\n",
    "        print(f'Ind:{i}')\n",
    "        ind = IndESN().create_random()\n",
    "        ind = self.get_fitness(ind)\n",
    "        return ind\n",
    "\n",
    "    def init_gen(self):\n",
    "        for i in range(self.n_generations):\n",
    "            print(f\"Iter: {i}\")\n",
    "\n",
    "            loc_pop = self.population[:self.n_individuals - 1].copy()\n",
    "\n",
    "            dask.compute(\n",
    "                [dask.delayed(self.crossover)(random.choice(loc_pop), random.choice(loc_pop)) for j in\n",
    "                 range(int(self.n_individuals))])\n",
    "\n",
    "            self.population = sorted(self.population, key=lambda a: a.fitness)\n",
    "            best = self.population[0]\n",
    "            self.iters.append(best)\n",
    "            self.save_iters_csv()\n",
    "\n",
    "            del loc_pop\n",
    "            gc.collect()\n",
    "            print(f\"Best: {self.population[0].fitness}\")\n",
    "\n",
    "    def crossover(self, ind_a, ind_b):\n",
    "        ind = IndESN()\n",
    "        ind.fitness = random.choice([ind_a.fitness, ind_b.fitness])\n",
    "        ind.n_reservoirs = random.choice([ind_a.n_reservoirs, ind_b.n_reservoirs])\n",
    "        ind.sparsity = random.choice([ind_a.sparsity, ind_b.sparsity])\n",
    "        ind.spectral_radius = random.choice([ind_a.spectral_radius, ind_b.spectral_radius])\n",
    "        ind.noise = random.choice([ind_a.noise, ind_b.noise])\n",
    "        if random.uniform(0, 1) < self.mutation_rate:\n",
    "            ind = self.mutation(ind)\n",
    "\n",
    "        ind = self.get_fitness(ind)\n",
    "        self.population.append(ind)\n",
    "        return ind\n",
    "\n",
    "    def mutation(self, ind):\n",
    "        random.choice([\n",
    "            ind.rand_n_reservoirs(),\n",
    "            ind.rand_sparsity(),\n",
    "            ind.rand_spectral_radius(),\n",
    "            ind.rand_noise()\n",
    "        ])\n",
    "        return ind\n",
    "\n",
    "    def get_fitness(self, individual):\n",
    "        search = list(filter(lambda ind:\n",
    "                             ind.fitness == individual.fitness and\n",
    "                             ind.n_reservoirs == individual.n_reservoirs and\n",
    "                             ind.sparsity == individual.sparsity and\n",
    "                             ind.spectral_radius == individual.spectral_radius and\n",
    "                             ind.noise == individual.noise, self.population))\n",
    "\n",
    "        if search:\n",
    "            return search[0]\n",
    "\n",
    "        try:\n",
    "\n",
    "            model = ESN(n_inputs=self.dataset[self.features].shape[1],\n",
    "                        n_outputs=1,\n",
    "                        n_reservoir=individual.n_reservoirs,\n",
    "                        sparsity=individual.sparsity,\n",
    "                        spectral_radius=individual.spectral_radius,\n",
    "                        noise=individual.noise,\n",
    "                        random_state=SEED)\n",
    "\n",
    "            cvs = []\n",
    "            subdf = {data: dados for data, dados in self.dataset.sort_values(\"CAMPUS\", ascending=True).groupby('DATA')}\n",
    "            for i_treino, i_teste in TimeSeriesSplit(n_splits=5, test_size=1).split(subdf):\n",
    "                i_treino = [list(subdf.keys())[index] for index in i_treino]\n",
    "                i_teste = [list(subdf.keys())[index] for index in i_teste]\n",
    "\n",
    "                dados_treino = pd.concat([subdf[index] for index in i_treino], ignore_index=True)\n",
    "                dados_teste = pd.concat([subdf[index] for index in i_teste], ignore_index=True)\n",
    "\n",
    "                x_treino, y_treino = dados_treino[self.features].astype(np.float32).to_cupy().get(), dados_treino[\n",
    "                    \"CONSUMO\"].astype(\n",
    "                    np.float32).to_cupy().get()\n",
    "                x_teste, y_teste = dados_teste[self.features].astype(np.float32).to_cupy().get(), dados_teste[\n",
    "                    \"CONSUMO\"].astype(\n",
    "                    np.float32).to_cupy().get()\n",
    "\n",
    "                model.fit(x_treino, y_treino)\n",
    "\n",
    "                y_previsto = []\n",
    "                for row in x_teste:\n",
    "                    previsao = model.predict(row.reshape(1, -1))[0]\n",
    "                    y_previsto.append(previsao)\n",
    "\n",
    "                mape = mean_absolute_percentage_error(y_teste, y_previsto)\n",
    "                cvs.append(mape)\n",
    "\n",
    "                del x_treino, y_treino, x_teste, y_teste, y_previsto\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n!!! Repeating objective function: {e}.\")\n",
    "            return self.get_fitness(individual)\n",
    "\n",
    "        individual.fitness = np.array(cvs).mean()\n",
    "\n",
    "        del cvs, model\n",
    "        gc.collect()\n",
    "        return individual\n",
    "\n",
    "    def iters_dataframe(self):\n",
    "        df = pandas.DataFrame()\n",
    "        for i in range(len(self.iters)):\n",
    "            ind = self.iters[i]\n",
    "            df = pandas.concat([df, pandas.DataFrame.from_dict({\n",
    "                \"Reservoirs\": ind.n_reservoirs,\n",
    "                \"Sparsity\": ind.sparsity,\n",
    "                \"Spectral Radius\": ind.spectral_radius,\n",
    "                \"Noise\": ind.noise,\n",
    "                \"Fitness\": ind.fitness,\n",
    "            }, orient='index').T], ignore_index=True)\n",
    "        return df\n",
    "\n",
    "    def save_iters_csv(self):\n",
    "        pd_df = self.iters_dataframe()\n",
    "        pd_df.to_csv(f\"parâmetros/GA-ESN ITERS SEED {self.seed}.csv\", sep=\";\", decimal=\",\", index=True)\n",
    "        del pd_df\n",
    "\n"
   ],
   "id": "6cb04944db95aedf",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LSTM",
   "id": "4fc1862951bbd874"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T20:03:49.899276Z",
     "start_time": "2025-01-25T20:03:49.152681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IndLSTM:\n",
    "    def __init__(self):\n",
    "        self.fitness = None\n",
    "        self.lstm_units = 0\n",
    "        self.epochs = 0\n",
    "        self.batch_size = 0\n",
    "        self.lstm_activation = None\n",
    "        self.bias = None\n",
    "\n",
    "    def create_random(self):\n",
    "        self.rand_units()\n",
    "        self.rand_epochs()\n",
    "        self.rand_batch()\n",
    "        self.rand_activation()\n",
    "        self.rand_bias()\n",
    "        return self\n",
    "\n",
    "    def rand_units(self):\n",
    "        self.lstm_units = random.randint(1, 300)\n",
    "\n",
    "    def rand_epochs(self):\n",
    "        self.epochs = random.randint(1, 100)\n",
    "\n",
    "    def rand_batch(self):\n",
    "        self.batch_size = random.randint(1, 300)\n",
    "\n",
    "    def rand_activation(self):\n",
    "        self.lstm_activation = random.choice(\n",
    "            [\"linear\", \"mish\", \"sigmoid\", \"softmax\", \"softplus\", \"softsign\", \"tanh\"])\n",
    "\n",
    "    def rand_bias(self):\n",
    "        self.bias = random.choice([False, True])\n",
    "\n",
    "\n",
    "class GALSTM:\n",
    "    def __init__(self, dataset, features, n_individuals, n_generations, seed=SEED):\n",
    "        reset_seed(seed)\n",
    "        self.seed = seed\n",
    "        self.dataset = dataset\n",
    "        self.features = features\n",
    "        self.n_individuals = n_individuals\n",
    "        self.n_generations = n_generations\n",
    "        self.mutation_rate = 0.5\n",
    "        self.population = []\n",
    "        self.iters = []\n",
    "        self.init_pop()\n",
    "        self.init_gen()\n",
    "\n",
    "    def init_pop(self):\n",
    "        for _ in range(self.n_individuals):\n",
    "            self.create_ind(_)\n",
    "        self.population = sorted(self.population, key=lambda a: a.fitness)\n",
    "        best = self.population[0]\n",
    "        self.iters.append(best)\n",
    "\n",
    "    def create_ind(self, i):\n",
    "        print(f'Ind:{i}')\n",
    "        ind = IndLSTM().create_random()\n",
    "        ind = self.get_fitness(ind)\n",
    "        self.population.append(ind)\n",
    "        return ind\n",
    "\n",
    "    def init_gen(self):\n",
    "        for i in range(self.n_generations):\n",
    "            print(f\"Iter: {i}\")\n",
    "            loc_pop = self.population[:self.n_individuals - 1].copy()\n",
    "\n",
    "            for j in range(int(self.n_individuals)):\n",
    "                self.crossover(random.choice(loc_pop), random.choice(loc_pop))\n",
    "\n",
    "            self.population = sorted(self.population, key=lambda a: a.fitness)\n",
    "            best = self.population[0]\n",
    "\n",
    "            self.iters.append(best)\n",
    "            self.save_iters_csv()\n",
    "\n",
    "            del loc_pop\n",
    "            gc.collect()\n",
    "            print(f\"Best: {self.population[0].fitness}\")\n",
    "\n",
    "    def crossover(self, ind_a, ind_b):\n",
    "        ind = IndLSTM()\n",
    "        ind.lstm_units = random.choice([ind_a.lstm_units, ind_b.lstm_units])\n",
    "        ind.epochs = random.choice([ind_a.epochs, ind_b.epochs])\n",
    "        ind.batch_size = random.choice([ind_a.batch_size, ind_b.batch_size])\n",
    "        ind.lstm_activation = random.choice([ind_a.lstm_activation, ind_b.lstm_activation])\n",
    "        ind.bias = random.choice([ind_a.bias, ind_b.bias])\n",
    "\n",
    "        if random.uniform(0, 1) < self.mutation_rate:\n",
    "            ind = self.mutation(ind)\n",
    "\n",
    "        ind = self.get_fitness(ind)\n",
    "        self.population.append(ind)\n",
    "        return ind\n",
    "\n",
    "    def mutation(self, ind):\n",
    "        random.choice([\n",
    "            ind.rand_units(),\n",
    "            ind.rand_epochs(),\n",
    "            ind.rand_batch(),\n",
    "            ind.rand_activation(),\n",
    "            ind.rand_bias()\n",
    "        ])\n",
    "        return ind\n",
    "\n",
    "    def get_fitness(self, individual):\n",
    "        print(f\"Units: {individual.lstm_units}\" +\n",
    "              f\"Epochs: {individual.epochs}\" +\n",
    "              f\"Batch Size: {individual.batch_size}\" +\n",
    "              f\"Activation: {individual.lstm_activation}\" +\n",
    "              f\"Bias: {individual.bias}\")\n",
    "\n",
    "        search = list(filter(lambda ind:\n",
    "                             ind.lstm_units == individual.lstm_units and\n",
    "                             ind.epochs == individual.epochs and\n",
    "                             ind.batch_size == individual.batch_size and\n",
    "                             ind.lstm_activation == individual.lstm_activation and\n",
    "                             ind.bias == individual.bias, self.population))\n",
    "\n",
    "        if search:\n",
    "            return search[0]\n",
    "\n",
    "        try:\n",
    "            tf.keras.backend.clear_session()\n",
    "            model = Sequential([\n",
    "                Input((self.dataset[self.features].shape[1], 1)),\n",
    "                LSTM(individual.lstm_units,\n",
    "                     activation=individual.lstm_activation,\n",
    "                     use_bias=individual.bias,\n",
    "                     seed=SEED),\n",
    "                Dense(1),\n",
    "            ])\n",
    "            model.compile(loss='mape')\n",
    "\n",
    "            cvs = []\n",
    "            subdf = {data: dados for data, dados in self.dataset.sort_values(\"CAMPUS\", ascending=True).groupby('DATA')}\n",
    "            for i_treino, i_teste in TimeSeriesSplit(n_splits=5, test_size=1).split(subdf):\n",
    "                i_treino = [list(subdf.keys())[index] for index in i_treino]\n",
    "                i_teste = [list(subdf.keys())[index] for index in i_teste]\n",
    "\n",
    "                dados_treino = pd.concat([subdf[index] for index in i_treino], ignore_index=True)\n",
    "                dados_teste = pd.concat([subdf[index] for index in i_teste], ignore_index=True)\n",
    "\n",
    "                x_treino, y_treino = dados_treino[self.features].astype(np.float32).to_cupy().get(), dados_treino[\n",
    "                    \"CONSUMO\"].astype(\n",
    "                    np.float32).to_cupy().get()\n",
    "                x_teste, y_teste = dados_teste[self.features].astype(np.float32).to_cupy().get(), dados_teste[\n",
    "                    \"CONSUMO\"].to_cupy().get()\n",
    "\n",
    "                model.fit(x_treino, y_treino, shuffle=False, verbose=False, epochs=individual.epochs,\n",
    "                          batch_size=individual.batch_size)\n",
    "\n",
    "                y_previsto = []\n",
    "                for row in x_teste:\n",
    "                    previsao = model.predict(row.reshape(1, -1))[0]\n",
    "                    y_previsto.append(previsao)\n",
    "\n",
    "                mape = mean_absolute_percentage_error(y_teste, y_previsto)\n",
    "                cvs.append(mape)\n",
    "                del x_treino, y_treino, x_teste, y_teste, y_previsto\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n!!! Repeating objective function: {e}.\")\n",
    "            return self.get_fitness(individual)\n",
    "\n",
    "        individual.fitness = np.array(cvs).mean()\n",
    "\n",
    "        del cvs, model\n",
    "        gc.collect()\n",
    "        return individual\n",
    "\n",
    "    def iters_dataframe(self):\n",
    "        df = pandas.DataFrame()\n",
    "        for i in range(len(self.iters)):\n",
    "            ind = self.iters[i]\n",
    "            df = pandas.concat([df, pandas.DataFrame.from_dict({\n",
    "                \"Units\": ind.lstm_units,\n",
    "                \"Epochs\": ind.epochs,\n",
    "                \"Batch Size\": ind.batch_size,\n",
    "                \"Activation\": ind.lstm_activation,\n",
    "                \"Bias\": ind.bias,\n",
    "                \"Fitness\": ind.fitness,\n",
    "            }, orient='index').T], ignore_index=True)\n",
    "        return df\n",
    "\n",
    "    def save_iters_csv(self):\n",
    "        pd_df = self.iters_dataframe()\n",
    "        pd_df.to_csv(f\"parâmetros/GA-LSTM ITERS SEED {self.seed}.csv\", sep=\";\", decimal=\",\", index=True)\n",
    "        del pd_df\n",
    "\n"
   ],
   "id": "b21bd98141376106",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Random Forest\n",
   "id": "c2aa056966154593"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T20:03:49.967825Z",
     "start_time": "2025-01-25T20:03:49.950081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IndRF:\n",
    "    def __init__(self):\n",
    "        self.fitness = None\n",
    "        self.estimators = 0\n",
    "        self.max_depth = 0\n",
    "        self.min_samples_split = 0\n",
    "        self.min_samples_leaf = 0\n",
    "\n",
    "    def create_random(self):\n",
    "        self.rand_estimators()\n",
    "        self.rand_depth()\n",
    "        self.rand_samples_split()\n",
    "        self.rand_samples_leaf()\n",
    "        return self\n",
    "\n",
    "    def rand_estimators(self):\n",
    "        self.estimators = random.randint(10, 300)\n",
    "\n",
    "    def rand_depth(self):\n",
    "        self.max_depth = random.randint(10, 300)\n",
    "\n",
    "    def rand_samples_split(self):\n",
    "        self.min_samples_split = random.randint(2, 50)\n",
    "\n",
    "    def rand_samples_leaf(self):\n",
    "        self.min_samples_leaf = random.randint(1, 50)\n",
    "\n",
    "\n",
    "class GARF:\n",
    "    def __init__(self, dataset, features, n_individuals, n_generations, seed=SEED):\n",
    "        reset_seed(seed)\n",
    "        self.seed = seed\n",
    "        self.dataset = dataset\n",
    "        self.features = features\n",
    "        self.n_individuals = n_individuals\n",
    "        self.n_generations = n_generations\n",
    "        self.mutation_rate = 0.5\n",
    "        self.population = {}\n",
    "        self.iters = []\n",
    "        self.init_pop()\n",
    "        self.init_gen()\n",
    "\n",
    "    def init_pop(self):\n",
    "        futures = [dask.delayed(self.create_ind)(_) for _ in range(self.n_individuals)]\n",
    "        self.population = dask.compute(futures)[0]\n",
    "        self.population = sorted(self.population, key=lambda a: a.fitness)\n",
    "        best = self.population[0]\n",
    "        self.iters.append(best)\n",
    "\n",
    "    def create_ind(self, i):\n",
    "        print(f'Ind:{i}')\n",
    "        ind = IndRF().create_random()\n",
    "        ind = self.get_fitness(ind)\n",
    "        return ind\n",
    "\n",
    "    def init_gen(self):\n",
    "        for i in range(self.n_generations):\n",
    "            print(f\"Iter: {i}\")\n",
    "\n",
    "            loc_pop = self.population[:self.n_individuals - 1].copy()\n",
    "\n",
    "            dask.compute(\n",
    "                [dask.delayed(self.crossover)(random.choice(loc_pop), random.choice(loc_pop)) for j in\n",
    "                 range(int(self.n_individuals))])\n",
    "\n",
    "            self.population = sorted(self.population, key=lambda a: a.fitness)\n",
    "            best = self.population[0]\n",
    "            self.iters.append(best)\n",
    "            self.save_iters_csv()\n",
    "\n",
    "            del loc_pop\n",
    "            gc.collect()\n",
    "            print(f\"Best: {self.population[0].fitness}\")\n",
    "\n",
    "    def crossover(self, ind_a, ind_b):\n",
    "        ind = IndRF()\n",
    "        ind.estimators = random.choice([ind_a.estimators, ind_b.estimators])\n",
    "        ind.max_depth = random.choice([ind_a.max_depth, ind_b.max_depth])\n",
    "        ind.min_samples_split = random.choice([ind_a.min_samples_split, ind_b.min_samples_split])\n",
    "        ind.min_samples_leaf = random.choice([ind_a.min_samples_leaf, ind_b.min_samples_leaf])\n",
    "\n",
    "        if random.uniform(0, 1) < self.mutation_rate:\n",
    "            ind = self.mutation(ind)\n",
    "\n",
    "        ind = self.get_fitness(ind)\n",
    "        self.population.append(ind)\n",
    "        return ind\n",
    "\n",
    "    def mutation(self, ind):\n",
    "        random.choice([\n",
    "            ind.rand_estimators(),\n",
    "            ind.rand_depth(),\n",
    "            ind.rand_samples_split(),\n",
    "            ind.rand_samples_leaf(),\n",
    "        ])\n",
    "        return ind\n",
    "\n",
    "    def get_fitness(self, individual):\n",
    "        search = list(filter(lambda ind:\n",
    "                             ind.estimators == individual.estimators and\n",
    "                             ind.max_depth == individual.max_depth and\n",
    "                             ind.min_samples_split == individual.min_samples_split and\n",
    "                             ind.min_samples_leaf == individual.min_samples_leaf, self.population))\n",
    "\n",
    "        if search:\n",
    "            return search[0]\n",
    "\n",
    "        try:\n",
    "            model = CudaRandomForest(random_state=SEED,\n",
    "                                     n_estimators=individual.estimators,\n",
    "                                     max_depth=individual.max_depth,\n",
    "                                     min_samples_split=individual.min_samples_split,\n",
    "                                     min_samples_leaf=individual.min_samples_leaf,\n",
    "                                     n_streams=individual.estimators,\n",
    "                                     n_bins=individual.min_samples_split)\n",
    "\n",
    "            cvs = []\n",
    "            subdf = {data: dados for data, dados in self.dataset.sort_values(\"CAMPUS\", ascending=True).groupby('DATA')}\n",
    "            for i_treino, i_teste in TimeSeriesSplit(n_splits=5, test_size=1).split(subdf):\n",
    "                i_treino = [list(subdf.keys())[index] for index in i_treino]\n",
    "                i_teste = [list(subdf.keys())[index] for index in i_teste]\n",
    "\n",
    "                dados_treino = pd.concat([subdf[index] for index in i_treino], ignore_index=True)\n",
    "                dados_teste = pd.concat([subdf[index] for index in i_teste], ignore_index=True)\n",
    "\n",
    "                x_treino, y_treino = dados_treino[self.features].astype(np.float32).to_cupy().get(), dados_treino[\n",
    "                    \"CONSUMO\"].astype(\n",
    "                    np.float32).to_cupy().get()\n",
    "                x_teste, y_teste = dados_teste[self.features].astype(np.float32).to_cupy().get(), dados_teste[\n",
    "                    \"CONSUMO\"].to_cupy().get()\n",
    "\n",
    "                model.fit(x_treino, y_treino)\n",
    "\n",
    "                y_previsto = []\n",
    "                for row in x_teste:\n",
    "                    previsao = model.predict(row.reshape(1, -1))\n",
    "                    y_previsto.append(previsao)\n",
    "                mape = mean_absolute_percentage_error(y_teste, y_previsto)\n",
    "                cvs.append(mape)\n",
    "\n",
    "                del x_treino, y_treino, x_teste, y_teste, y_previsto\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n!!! Repeating objective function: {e}.\")\n",
    "            return self.get_fitness(individual)\n",
    "\n",
    "        individual.fitness = np.array(cvs).mean()\n",
    "\n",
    "        del cvs, model\n",
    "        gc.collect()\n",
    "        return individual\n",
    "\n",
    "    def iters_dataframe(self):\n",
    "        df = pandas.DataFrame()\n",
    "        for i in range(len(self.iters)):\n",
    "            ind = self.iters[i]\n",
    "            df = pandas.concat([df, pandas.DataFrame.from_dict({\n",
    "                \"N_estimators\": ind.estimators,\n",
    "                \"Max_depth\": ind.max_depth,\n",
    "                \"Min_samples_split\": ind.min_samples_split,\n",
    "                \"Min_samples_leaf\": ind.min_samples_leaf,\n",
    "                \"Fitness\": ind.fitness,\n",
    "                \"Base Seed\": self.seed,\n",
    "            }, orient='index').T], ignore_index=True)\n",
    "        return df\n",
    "\n",
    "    def save_iters_csv(self):\n",
    "        pd_df = self.iters_dataframe()\n",
    "        pd_df.to_csv(f\"parâmetros/GA-RF ITERS SEED {self.seed}.csv\", sep=\";\", decimal=\",\", index=True)\n",
    "        del pd_df\n",
    "\n"
   ],
   "id": "b08401fbaa2e36f5",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### XGB",
   "id": "457e15a0d79cdca8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T20:03:50.040832Z",
     "start_time": "2025-01-25T20:03:50.025254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IndXGB:\n",
    "    def __init__(self):\n",
    "        self.fitness = None\n",
    "        self.estimators = 0\n",
    "        self.max_depth = 0\n",
    "        self.booster = None\n",
    "        self.reg_lambda = 0\n",
    "        self.reg_alpha = 0\n",
    "\n",
    "    def create_random(self):\n",
    "        self.rand_estimators()\n",
    "        self.rand_depth()\n",
    "        self.rand_booster()\n",
    "        self.rand_lambda()\n",
    "        self.rand_alpha()\n",
    "        return self\n",
    "\n",
    "    def rand_estimators(self):\n",
    "        self.estimators = random.randint(1, 300)\n",
    "\n",
    "    def rand_depth(self):\n",
    "        self.max_depth = random.randint(1, 300)\n",
    "\n",
    "    def rand_booster(self):\n",
    "        self.booster = random.choice([\"gbtree\", \"gblinear\", \"dart\"])\n",
    "\n",
    "    def rand_lambda(self):\n",
    "        self.reg_lambda = random.uniform(0, 100)\n",
    "\n",
    "    def rand_alpha(self):\n",
    "        self.reg_alpha = random.uniform(0, 100)\n",
    "\n",
    "\n",
    "class GAXGB:\n",
    "    def __init__(self, dataset, features, n_individuals, n_generations, seed=SEED):\n",
    "        reset_seed(seed)\n",
    "        self.seed = seed\n",
    "        self.dataset = dataset\n",
    "        self.features = features\n",
    "        self.n_individuals = n_individuals\n",
    "        self.n_generations = n_generations\n",
    "        self.mutation_rate = 0.5\n",
    "        self.population = []\n",
    "        self.iters = []\n",
    "        self.init_pop()\n",
    "        self.init_gen()\n",
    "\n",
    "    def init_pop(self):\n",
    "        futures = [dask.delayed(self.create_ind)(_) for _ in range(self.n_individuals)]\n",
    "        self.population = dask.compute(futures)[0]\n",
    "        self.population = sorted(self.population, key=lambda a: a.fitness)\n",
    "        best = self.population[0]\n",
    "        self.iters.append(best)\n",
    "\n",
    "    def create_ind(self, i):\n",
    "        print(f'Ind:{i}')\n",
    "        ind = IndXGB().create_random()\n",
    "        ind = self.get_fitness(ind)\n",
    "        return ind\n",
    "\n",
    "    def init_gen(self):\n",
    "        for i in range(self.n_generations):\n",
    "            print(f\"Iter: {i}\")\n",
    "\n",
    "            loc_pop = self.population[:self.n_individuals - 1].copy()\n",
    "\n",
    "            dask.compute(\n",
    "                [dask.delayed(self.crossover)(random.choice(loc_pop), random.choice(loc_pop)) for j in\n",
    "                 range(int(self.n_individuals))])\n",
    "\n",
    "            self.population = sorted(self.population, key=lambda a: a.fitness)\n",
    "            best = self.population[0]\n",
    "            self.iters.append(best)\n",
    "            self.save_iters_csv()\n",
    "\n",
    "            del loc_pop\n",
    "            gc.collect()\n",
    "            print(f\"Best: {self.population[0].fitness}\")\n",
    "\n",
    "    def crossover(self, ind_a, ind_b):\n",
    "        ind = IndXGB()\n",
    "        ind.estimators = random.choice([ind_a.estimators, ind_b.estimators])\n",
    "        ind.max_depth = random.choice([ind_a.max_depth, ind_b.max_depth])\n",
    "        ind.booster = random.choice([ind_a.booster, ind_b.booster])\n",
    "        ind.reg_lambda = random.choice([ind_a.reg_lambda, ind_b.reg_lambda])\n",
    "        ind.reg_alpha = random.choice([ind_a.reg_alpha, ind_b.reg_alpha])\n",
    "\n",
    "        if random.uniform(0, 1) < self.mutation_rate:\n",
    "            ind = self.mutation(ind)\n",
    "\n",
    "        ind = self.get_fitness(ind)\n",
    "        self.population.append(ind)\n",
    "        return ind\n",
    "\n",
    "    def mutation(self, ind):\n",
    "        random.choice([\n",
    "            ind.rand_estimators(),\n",
    "            ind.rand_depth(),\n",
    "            ind.rand_booster(),\n",
    "            ind.rand_lambda(),\n",
    "            ind.rand_alpha()\n",
    "        ])\n",
    "        return ind\n",
    "\n",
    "    def get_fitness(self, individual):\n",
    "        search = list(filter(lambda ind:\n",
    "                             ind.estimators == individual.estimators and\n",
    "                             ind.max_depth == individual.max_depth and\n",
    "                             ind.booster == individual.booster and\n",
    "                             ind.reg_lambda == individual.reg_lambda and\n",
    "                             ind.reg_alpha == individual.reg_alpha, self.population))\n",
    "\n",
    "        if search:\n",
    "            return search[0]\n",
    "\n",
    "        updater = \"coord_descent\" if individual.booster == \"gblinear\" else None\n",
    "\n",
    "        try:\n",
    "            model = XGBRegressor(device=\"cuda\", random_state=SEED,\n",
    "                                 n_estimators=individual.estimators,\n",
    "                                 max_depth=individual.max_depth,\n",
    "                                 booster=individual.booster,\n",
    "                                 reg_lambda=individual.reg_lambda,\n",
    "                                 reg_alpha=individual.reg_alpha,\n",
    "                                 updater=updater)\n",
    "\n",
    "            cvs = []\n",
    "            subdf = {data: dados for data, dados in self.dataset.sort_values(\"CAMPUS\", ascending=True).groupby('DATA')}\n",
    "            for i_treino, i_teste in TimeSeriesSplit(n_splits=5, test_size=1).split(subdf):\n",
    "                i_treino = [list(subdf.keys())[index] for index in i_treino]\n",
    "                i_teste = [list(subdf.keys())[index] for index in i_teste]\n",
    "\n",
    "                dados_treino = pd.concat([subdf[index] for index in i_treino], ignore_index=True)\n",
    "                dados_teste = pd.concat([subdf[index] for index in i_teste], ignore_index=True)\n",
    "\n",
    "                x_treino, y_treino = dados_treino[self.features].astype(np.float32).to_cupy().get(), dados_treino[\n",
    "                    \"CONSUMO\"].astype(\n",
    "                    np.float32).to_cupy().get()\n",
    "                x_teste, y_teste = dados_teste[self.features].astype(np.float32).to_cupy().get(), dados_teste[\n",
    "                    \"CONSUMO\"].to_cupy().get()\n",
    "\n",
    "                model.fit(x_treino, y_treino)\n",
    "\n",
    "                y_previsto = []\n",
    "                for row in x_teste:\n",
    "                    previsao = model.predict(row.reshape(1, -1))\n",
    "                    y_previsto.append(previsao)\n",
    "                mape = mean_absolute_percentage_error(y_teste, y_previsto)\n",
    "                cvs.append(mape)\n",
    "\n",
    "                del x_treino, y_treino, x_teste, y_teste, y_previsto\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n!!! Repeating objective function: {e}.\")\n",
    "            return self.get_fitness(individual)\n",
    "\n",
    "        individual.fitness = np.array(cvs).mean()\n",
    "\n",
    "        del cvs, model\n",
    "        gc.collect()\n",
    "        return individual\n",
    "\n",
    "    def iters_dataframe(self):\n",
    "        df = pandas.DataFrame()\n",
    "        for i in range(len(self.iters)):\n",
    "            ind = self.iters[i]\n",
    "            df = pandas.concat([df, pandas.DataFrame.from_dict({\n",
    "                \"N_estimators\": ind.estimators,\n",
    "                \"Max_depth\": ind.max_depth,\n",
    "                \"Booster\": ind.booster,\n",
    "                \"Lambda\": ind.reg_lambda,\n",
    "                \"Alpha\": ind.reg_alpha,\n",
    "                \"Fitness\": ind.fitness,\n",
    "                \"Base Seed\": self.seed,\n",
    "            }, orient='index').T], ignore_index=True)\n",
    "        return df\n",
    "\n",
    "    def save_iters_csv(self):\n",
    "        pd_df = self.iters_dataframe()\n",
    "        pd_df.to_csv(f\"parâmetros/GA-XGB ITERS SEED {self.seed}.csv\", sep=\";\", decimal=\",\", index=True)\n",
    "        del pd_df\n"
   ],
   "id": "c59fe372420d8cc8",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Particle Swarm Optimization",
   "id": "1331ff48ed0b7c5b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ESN",
   "id": "e7eb94590843ecc2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T20:03:50.113268Z",
     "start_time": "2025-01-25T20:03:50.102096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PartESN:\n",
    "    def __init_(self):\n",
    "        self.fitness = None\n",
    "        self.n_reservoirs = 0\n",
    "        self.sparsity = 0\n",
    "        self.spectral_radius = 0\n",
    "        self.noise = 0\n",
    "\n",
    "\n",
    "class PSOESN:\n",
    "    def __init__(self, dataset, features, n_particles, n_iters, seed=SEED):\n",
    "        reset_seed(seed)\n",
    "        self.seed = seed\n",
    "        self.dataset = dataset\n",
    "        self.features = features\n",
    "        self.n_particles = n_particles\n",
    "        self.n_iters = n_iters\n",
    "        self.particles = []\n",
    "        self.iters = []\n",
    "        self.run()\n",
    "\n",
    "    def run(self):\n",
    "        lower_bound = [2, 0.1, 0.1, 0.0001]\n",
    "        uppper_bound = [1000, 0.5, 1.9, 0.8]\n",
    "        bounds = (lower_bound, uppper_bound)\n",
    "\n",
    "        options = {'c1': 0.5, 'c2': 0.5, 'w': 0.5}\n",
    "        optimizer = GlobalBestPSO(n_particles=self.n_particles,\n",
    "                                  dimensions=4,\n",
    "                                  options=options,\n",
    "                                  bounds=bounds)\n",
    "\n",
    "        optimizer.optimize(self.get_fitness, iters=self.n_iters)\n",
    "        self.particles = sorted(self.particles, key=lambda a: a.fitness)\n",
    "\n",
    "    def get_fitness(self, parts):\n",
    "        parts = np.round(parts)\n",
    "\n",
    "        fit_lst = []\n",
    "        for j in range(self.n_particles):\n",
    "            fit_lst.append(self.objective_function(parts[j]))\n",
    "\n",
    "        self.particles = sorted(self.particles, key=lambda a: a.fitness)\n",
    "        best = self.particles[0]\n",
    "        self.iters.append(best)\n",
    "        self.save_iters_csv()\n",
    "\n",
    "        gc.collect()\n",
    "        return fit_lst\n",
    "\n",
    "    def objective_function(self, particle_arr):\n",
    "        particle = PartESN()\n",
    "        particle.n_reservoirs = int(particle_arr[0])\n",
    "        particle.sparsity = particle_arr[1]\n",
    "        particle.spectral_radius = particle_arr[2]\n",
    "        particle.noise = particle_arr[3]\n",
    "\n",
    "        search = list(filter(lambda par:\n",
    "                             par.n_reservoirs == particle.n_reservoirs and\n",
    "                             par.sparsity == particle.sparsity and\n",
    "                             par.spectral_radius == particle.spectral_radius and\n",
    "                             par.noise == particle.noise, self.particles))\n",
    "\n",
    "        if search:\n",
    "            self.particles.append(search[0])\n",
    "            return search[0].fitness\n",
    "\n",
    "        try:\n",
    "            model = ESN(n_inputs=self.dataset[self.features].shape[1],\n",
    "                        n_outputs=1,\n",
    "                        n_reservoir=particle.n_reservoirs,\n",
    "                        sparsity=particle.sparsity,\n",
    "                        spectral_radius=particle.spectral_radius,\n",
    "                        noise=particle.noise,\n",
    "                        random_state=SEED)\n",
    "\n",
    "            cvs = []\n",
    "            subdf = {data: dados for data, dados in self.dataset.sort_values(\"CAMPUS\", ascending=True).groupby('DATA')}\n",
    "            for i_treino, i_teste in TimeSeriesSplit(n_splits=5, test_size=1).split(subdf):\n",
    "                i_treino = [list(subdf.keys())[index] for index in i_treino]\n",
    "                i_teste = [list(subdf.keys())[index] for index in i_teste]\n",
    "\n",
    "                dados_treino = pd.concat([subdf[index] for index in i_treino], ignore_index=True)\n",
    "                dados_teste = pd.concat([subdf[index] for index in i_teste], ignore_index=True)\n",
    "\n",
    "                x_treino, y_treino = dados_treino[self.features].astype(np.float32).to_cupy().get(), dados_treino[\n",
    "                    \"CONSUMO\"].astype(\n",
    "                    np.float32).to_cupy().get()\n",
    "                x_teste, y_teste = dados_teste[self.features].astype(np.float32).to_cupy().get(), dados_teste[\n",
    "                    \"CONSUMO\"].to_cupy().get()\n",
    "\n",
    "                model.fit(x_treino, y_treino)\n",
    "\n",
    "                y_previsto = []\n",
    "                for row in x_teste:\n",
    "                    previsao = model.predict(row.reshape(1, -1))[0]\n",
    "                    y_previsto.append(previsao)\n",
    "                mape = mean_absolute_percentage_error(y_teste, y_previsto)\n",
    "                cvs.append(mape)\n",
    "\n",
    "                del x_treino, y_treino, x_teste, y_teste, y_previsto\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n!!! Repeating objective function: {e}.\")\n",
    "            return self.get_fitness(particle)\n",
    "\n",
    "        particle.fitness = np.array(cvs).mean()\n",
    "\n",
    "        self.particles.append(particle)\n",
    "\n",
    "        del cvs, model\n",
    "        gc.collect()\n",
    "        return particle.fitness\n",
    "\n",
    "    def iters_dataframe(self):\n",
    "        df = pandas.DataFrame()\n",
    "        for i in range(len(self.iters)):\n",
    "            part = self.iters[i]\n",
    "            df = pandas.concat([df, pandas.DataFrame.from_dict({\n",
    "                \"Reservoirs\": part.n_reservoirs,\n",
    "                \"Sparsity\": part.sparsity,\n",
    "                \"Spectral Radius\": part.spectral_radius,\n",
    "                \"Noise\": part.noise,\n",
    "                \"Fitness\": part.fitness,\n",
    "            }, orient='index').T], ignore_index=True)\n",
    "        return df\n",
    "\n",
    "    def save_iters_csv(self):\n",
    "        pd_df = self.iters_dataframe()\n",
    "        pd_df.to_csv(f\"parâmetros/PSO-ESN ITERS SEED {self.seed}.csv\", sep=\";\", decimal=\",\", index=True)\n",
    "        del pd_df\n",
    "\n"
   ],
   "id": "d01dceb7dc934827",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### LSTM"
   ],
   "id": "d479b0a0e9127044"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T20:03:50.176057Z",
     "start_time": "2025-01-25T20:03:50.163534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PartLSTM:\n",
    "    def __init_(self):\n",
    "        self.fitness = None\n",
    "        self.lstm_units = 0\n",
    "        self.epochs = 0\n",
    "        self.batch_size = 0\n",
    "        self.lstm_activation = None\n",
    "        self.bias = None\n",
    "\n",
    "\n",
    "class PSOLSTM:\n",
    "    def __init__(self, dataset, features, n_particles, n_iters, seed=SEED):\n",
    "        reset_seed(seed)\n",
    "        self.seed = seed\n",
    "        self.dataset = dataset\n",
    "        self.features = features\n",
    "        self.n_particles = n_particles\n",
    "        self.n_iters = n_iters\n",
    "        self.particles = []\n",
    "        self.iters = []\n",
    "        self.ACTIVATIONS = [\"linear\", \"mish\", \"sigmoid\", \"softmax\", \"softplus\", \"softsign\", \"tanh\"]\n",
    "        self.BIAS = [False, True]\n",
    "        self.run()\n",
    "\n",
    "    def run(self):\n",
    "        lower_bound = [1, 1, 1, 0, 0]\n",
    "        uppper_bound = [300, 100, 300, 6, 1]\n",
    "        bounds = (lower_bound, uppper_bound)\n",
    "\n",
    "        options = {'c1': 0.5, 'c2': 0.5, 'w': 0.5}\n",
    "        optimizer = GlobalBestPSO(n_particles=self.n_particles,\n",
    "                                  dimensions=5,\n",
    "                                  options=options,\n",
    "                                  bounds=bounds)\n",
    "\n",
    "        optimizer.optimize(self.get_fitness, iters=self.n_iters)\n",
    "        self.particles = sorted(self.particles, key=lambda a: a.fitness)\n",
    "\n",
    "    def get_fitness(self, parts):\n",
    "        parts = np.round(parts)\n",
    "\n",
    "        fit_lst = []\n",
    "        for j in range(self.n_particles):\n",
    "            fit_lst.append(self.objective_function(parts[j]))\n",
    "\n",
    "        self.particles = sorted(self.particles, key=lambda a: a.fitness)\n",
    "        best = self.particles[0]\n",
    "        self.iters.append(best)\n",
    "        self.save_iters_csv()\n",
    "\n",
    "        gc.collect()\n",
    "        return fit_lst\n",
    "\n",
    "    def objective_function(self, particle_arr):\n",
    "        particle = PartLSTM()\n",
    "        particle.lstm_units = int(particle_arr[0])\n",
    "        particle.epochs = int(particle_arr[1])\n",
    "        particle.batch_size = int(particle_arr[2])\n",
    "        particle.lstm_activation = self.ACTIVATIONS[int(particle_arr[3])]\n",
    "        particle.bias = self.BIAS[int(particle_arr[4])]\n",
    "\n",
    "        search = list(filter(lambda par:\n",
    "                             par.lstm_units == particle.lstm_units and\n",
    "                             par.epochs == particle.epochs and\n",
    "                             par.batch_size == particle.batch_size and\n",
    "                             par.lstm_activation == particle.lstm_activation and\n",
    "                             par.bias == particle.bias, self.particles))\n",
    "\n",
    "        if search:\n",
    "            self.particles.append(search[0])\n",
    "            return search[0].fitness\n",
    "\n",
    "        try:\n",
    "            tf.keras.backend.clear_session()\n",
    "            model = Sequential([\n",
    "                Input((self.dataset[self.features].shape[1], 1)),\n",
    "                LSTM(particle.lstm_units,\n",
    "                     activation=particle.lstm_activation,\n",
    "                     use_bias=particle.bias,\n",
    "                     seed=SEED),\n",
    "                Dense(1),\n",
    "            ])\n",
    "            model.compile(loss='mape')\n",
    "\n",
    "            cvs = []\n",
    "            subdf = {data: dados for data, dados in self.dataset.sort_values(\"CAMPUS\", ascending=True).groupby('DATA')}\n",
    "            for i_treino, i_teste in TimeSeriesSplit(n_splits=5, test_size=1).split(subdf):\n",
    "                i_treino = [list(subdf.keys())[index] for index in i_treino]\n",
    "                i_teste = [list(subdf.keys())[index] for index in i_teste]\n",
    "\n",
    "                dados_treino = pd.concat([subdf[index] for index in i_treino], ignore_index=True)\n",
    "                dados_teste = pd.concat([subdf[index] for index in i_teste], ignore_index=True)\n",
    "\n",
    "                x_treino, y_treino = dados_treino[self.features].astype(np.float32).to_cupy().get(), dados_treino[\n",
    "                    \"CONSUMO\"].astype(\n",
    "                    np.float32).to_cupy().get()\n",
    "                x_teste, y_teste = dados_teste[self.features].astype(np.float32).to_cupy().get(), dados_teste[\n",
    "                    \"CONSUMO\"].to_cupy().get()\n",
    "\n",
    "                model.fit(x_treino, y_treino, shuffle=False, verbose=False, epochs=particle.epochs,\n",
    "                          batch_size=particle.batch_size)\n",
    "\n",
    "                y_previsto = []\n",
    "                for row in x_teste:\n",
    "                    previsao = model.predict(row.reshape(1, -1))[0]\n",
    "                    y_previsto.append(previsao)\n",
    "                mape = mean_absolute_percentage_error(y_teste, y_previsto)\n",
    "                cvs.append(mape)\n",
    "\n",
    "                del x_treino, y_treino, x_teste, y_teste, y_previsto\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n!!! Repeating objective function: {e}.\")\n",
    "            return self.objective_function(particle_arr)\n",
    "\n",
    "        particle.fitness = np.array(cvs).mean()\n",
    "\n",
    "        self.particles.append(particle)\n",
    "\n",
    "        del cvs, model\n",
    "        gc.collect()\n",
    "        return particle.fitness\n",
    "\n",
    "    def iters_dataframe(self):\n",
    "        df = pandas.DataFrame()\n",
    "        for i in range(len(self.iters)):\n",
    "            part = self.iters[i]\n",
    "            df = pandas.concat([df, pandas.DataFrame.from_dict({\n",
    "                \"Units\": part.lstm_units,\n",
    "                \"Epochs\": part.epochs,\n",
    "                \"Batch Size\": part.batch_size,\n",
    "                \"Activation\": part.lstm_activation,\n",
    "                \"Bias\": part.bias,\n",
    "                \"Fitness\": part.fitness,\n",
    "            }, orient='index').T], ignore_index=True)\n",
    "        return df\n",
    "\n",
    "    def save_iters_csv(self):\n",
    "        pd_df = self.iters_dataframe()\n",
    "        pd_df.to_csv(f\"parâmetros/PSO-LSTM ITERS SEED {self.seed}.csv\", sep=\";\", decimal=\",\", index=True)\n",
    "        del pd_df\n",
    "\n"
   ],
   "id": "ba11cc46764e56ba",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Random Forest\n",
   "id": "1fd3360624b9bd82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T20:03:50.234154Z",
     "start_time": "2025-01-25T20:03:50.221831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PartRF:\n",
    "    def __init_(self):\n",
    "        self.fitness = None\n",
    "        self.estimators = 0\n",
    "        self.max_depth = 0\n",
    "        self.min_samples_split = 0\n",
    "        self.min_samples_leaf = 0\n",
    "\n",
    "\n",
    "class PSORF:\n",
    "    def __init__(self, dataset, features, n_particles, n_iters, seed=SEED):\n",
    "        reset_seed(seed)\n",
    "        self.seed = seed\n",
    "        self.dataset = dataset\n",
    "        self.features = features\n",
    "        self.n_particles = n_particles\n",
    "        self.n_iters = n_iters\n",
    "        self.particles = []\n",
    "        self.iters = []\n",
    "        self.run()\n",
    "\n",
    "    def run(self):\n",
    "        lower_bound = [10, 10, 2, 1]\n",
    "        uppper_bound = [300, 300, 50, 50]\n",
    "        bounds = (lower_bound, uppper_bound)\n",
    "\n",
    "        options = {'c1': 0.5, 'c2': 0.5, 'w': 0.5}\n",
    "        optimizer = GlobalBestPSO(n_particles=self.n_particles,\n",
    "                                  dimensions=4,\n",
    "                                  options=options,\n",
    "                                  bounds=bounds)\n",
    "\n",
    "        optimizer.optimize(self.get_fitness, iters=self.n_iters)\n",
    "        self.particles = sorted(self.particles, key=lambda a: a.fitness)\n",
    "\n",
    "    def get_fitness(self, parts):\n",
    "        parts = np.round(parts)\n",
    "        iter = int(len(self.particles) / self.n_particles)\n",
    "\n",
    "        fit_lst = \\\n",
    "            dask.compute([dask.delayed(self.objective_function)(parts[j], iter) for j in range(self.n_particles)])[0]\n",
    "\n",
    "        self.particles = sorted(self.particles, key=lambda a: a.fitness)\n",
    "        best = self.particles[0]\n",
    "        self.iters.append(best)\n",
    "        self.save_iters_csv()\n",
    "\n",
    "        return fit_lst\n",
    "\n",
    "    def objective_function(self, particle_arr, i):\n",
    "        particle = PartRF()\n",
    "        particle.estimators = int(particle_arr[0])\n",
    "        particle.max_depth = int(particle_arr[1])\n",
    "        particle.min_samples_split = int(particle_arr[2])\n",
    "        particle.min_samples_leaf = int(particle_arr[3])\n",
    "\n",
    "        search = list(filter(lambda par:\n",
    "                             par.estimators == particle.estimators and\n",
    "                             par.max_depth == particle.max_depth and\n",
    "                             par.min_samples_split == particle.min_samples_split and\n",
    "                             par.min_samples_leaf == particle.min_samples_leaf, self.particles))\n",
    "\n",
    "        if search:\n",
    "            self.particles.append(search[0])\n",
    "            return search[0].fitness\n",
    "\n",
    "        try:\n",
    "            model = CudaRandomForest(random_state=SEED,\n",
    "                                     n_estimators=particle.estimators,\n",
    "                                     max_depth=particle.max_depth,\n",
    "                                     min_samples_split=particle.min_samples_split,\n",
    "                                     min_samples_leaf=particle.min_samples_leaf,\n",
    "                                     n_streams=particle.estimators,\n",
    "                                     n_bins=particle.min_samples_split)\n",
    "\n",
    "            cvs = []\n",
    "            subdf = {data: dados for data, dados in self.dataset.sort_values(\"CAMPUS\", ascending=True).groupby('DATA')}\n",
    "            for i_treino, i_teste in TimeSeriesSplit(n_splits=5, test_size=1).split(subdf):\n",
    "                i_treino = [list(subdf.keys())[index] for index in i_treino]\n",
    "                i_teste = [list(subdf.keys())[index] for index in i_teste]\n",
    "\n",
    "                dados_treino = pd.concat([subdf[index] for index in i_treino], ignore_index=True)\n",
    "                dados_teste = pd.concat([subdf[index] for index in i_teste], ignore_index=True)\n",
    "\n",
    "                x_treino, y_treino = dados_treino[self.features].astype(np.float32).to_cupy().get(), dados_treino[\n",
    "                    \"CONSUMO\"].astype(\n",
    "                    np.float32).to_cupy().get()\n",
    "                x_teste, y_teste = dados_teste[self.features].astype(np.float32).to_cupy().get(), dados_teste[\n",
    "                    \"CONSUMO\"].to_cupy().get()\n",
    "\n",
    "                model.fit(x_treino, y_treino)\n",
    "\n",
    "                y_previsto = []\n",
    "                for row in x_teste:\n",
    "                    previsao = model.predict(row.reshape(1, -1))\n",
    "                    y_previsto.append(previsao)\n",
    "                mape = mean_absolute_percentage_error(y_teste, y_previsto)\n",
    "                cvs.append(mape)\n",
    "\n",
    "                del x_treino, y_treino, x_teste, y_teste, y_previsto\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n!!! Repeating objective function: {e}.\")\n",
    "            return self.objective_function(particle_arr, i)\n",
    "\n",
    "        particle.fitness = np.array(cvs).mean()\n",
    "\n",
    "        self.particles.append(particle)\n",
    "\n",
    "        del cvs, model\n",
    "        gc.collect()\n",
    "        return particle.fitness\n",
    "\n",
    "    def iters_dataframe(self):\n",
    "        df = pandas.DataFrame()\n",
    "        for i in range(len(self.iters)):\n",
    "            part = self.iters[i]\n",
    "            df = pandas.concat([df, pandas.DataFrame.from_dict({\n",
    "                \"N_estimators\": part.estimators,\n",
    "                \"Max_depth\": part.max_depth,\n",
    "                \"Min_samples_split\": part.min_samples_split,\n",
    "                \"Min_samples_leaf\": part.min_samples_leaf,\n",
    "                \"Fitness\": part.fitness,\n",
    "            }, orient='index').T], ignore_index=True)\n",
    "        return df\n",
    "\n",
    "    def save_iters_csv(self):\n",
    "        pd_df = self.iters_dataframe()\n",
    "        pd_df.to_csv(f\"parâmetros/PSO-RF ITERS SEED {self.seed}.csv\", sep=\";\", decimal=\",\", index=True)\n",
    "        del pd_df\n",
    "\n"
   ],
   "id": "375501d33b0c5429",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### XGB",
   "id": "eb8f07077663f73b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T20:03:50.347845Z",
     "start_time": "2025-01-25T20:03:50.300316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PartXGB:\n",
    "    def __init_(self):\n",
    "        self.fitness = None\n",
    "        self.estimators = 0\n",
    "        self.max_depth = 0\n",
    "        self.booster = None\n",
    "        self.reg_lambda = 0\n",
    "        self.reg_alpha = 0\n",
    "\n",
    "\n",
    "class PSOXGB:\n",
    "    def __init__(self, dataset, features, n_particles, n_iters, seed=SEED):\n",
    "        reset_seed(seed)\n",
    "        self.seed = seed\n",
    "        self.dataset = dataset\n",
    "        self.features = features\n",
    "        self.n_particles = n_particles\n",
    "        self.n_iters = n_iters\n",
    "        self.particles = []\n",
    "        self.iters = []\n",
    "        self.BOOSTERS = [\"gbtree\", \"gblinear\", \"dart\"]\n",
    "        self.run()\n",
    "\n",
    "    def run(self):\n",
    "        lower_bound = [1, 1, 0, 0, 0]\n",
    "        uppper_bound = [300, 300, 2, 100, 100]\n",
    "        bounds = (lower_bound, uppper_bound)\n",
    "\n",
    "        options = {'c1': 0.5, 'c2': 0.5, 'w': 0.5}\n",
    "        optimizer = GlobalBestPSO(n_particles=self.n_particles,\n",
    "                                  dimensions=5,\n",
    "                                  options=options,\n",
    "                                  bounds=bounds)\n",
    "\n",
    "        optimizer.optimize(self.get_fitness, iters=self.n_iters)\n",
    "        self.particles = sorted(self.particles, key=lambda a: a.fitness)\n",
    "\n",
    "    def get_fitness(self, parts):\n",
    "        parts = np.round(parts)\n",
    "        iter = int(len(self.particles) / self.n_particles)\n",
    "\n",
    "        fit_lst = \\\n",
    "            dask.compute([dask.delayed(self.objective_function)(parts[j], iter) for j in range(self.n_particles)])[0]\n",
    "\n",
    "        self.particles = sorted(self.particles, key=lambda a: a.fitness)\n",
    "        best = self.particles[0]\n",
    "        self.iters.append(best)\n",
    "        self.save_iters_csv()\n",
    "\n",
    "        gc.collect()\n",
    "        return fit_lst\n",
    "\n",
    "    def objective_function(self, particle_arr, i):\n",
    "        particle = PartXGB()\n",
    "        particle.estimators = int(particle_arr[0])\n",
    "        particle.max_depth = int(particle_arr[1])\n",
    "        particle.booster = self.BOOSTERS[int(particle_arr[2])]\n",
    "        particle.reg_lambda = float(particle_arr[3])\n",
    "        particle.reg_alpha = float(particle_arr[4])\n",
    "\n",
    "        search = list(filter(lambda par:\n",
    "                             par.estimators == particle.estimators and\n",
    "                             par.max_depth == particle.max_depth and\n",
    "                             par.booster == particle.booster and\n",
    "                             par.reg_lambda == particle.reg_lambda and\n",
    "                             par.reg_alpha == particle.reg_alpha, self.particles))\n",
    "\n",
    "        if search:\n",
    "            self.particles.append(search[0])\n",
    "            return search[0].fitness\n",
    "\n",
    "        try:\n",
    "            updater = \"coord_descent\" if particle.booster == \"gblinear\" else None\n",
    "            model = XGBRegressor(device=\"cuda\", random_state=SEED,\n",
    "                                 n_estimators=particle.estimators,\n",
    "                                 max_depth=particle.max_depth,\n",
    "                                 booster=particle.booster,\n",
    "                                 reg_lambda=particle.reg_lambda,\n",
    "                                 reg_alpha=particle.reg_alpha,\n",
    "                                 updater=updater)\n",
    "\n",
    "            cvs = []\n",
    "            subdf = {data: dados for data, dados in self.dataset.sort_values(\"CAMPUS\", ascending=True).groupby('DATA')}\n",
    "            for i_treino, i_teste in TimeSeriesSplit(n_splits=5, test_size=1).split(subdf):\n",
    "                i_treino = [list(subdf.keys())[index] for index in i_treino]\n",
    "                i_teste = [list(subdf.keys())[index] for index in i_teste]\n",
    "\n",
    "                dados_treino = pd.concat([subdf[index] for index in i_treino], ignore_index=True)\n",
    "                dados_teste = pd.concat([subdf[index] for index in i_teste], ignore_index=True)\n",
    "\n",
    "                x_treino, y_treino = dados_treino[self.features].astype(np.float32).to_cupy().get(), dados_treino[\n",
    "                    \"CONSUMO\"].astype(\n",
    "                    np.float32).to_cupy().get()\n",
    "                x_teste, y_teste = dados_teste[self.features].astype(np.float32).to_cupy().get(), dados_teste[\n",
    "                    \"CONSUMO\"].to_cupy().get()\n",
    "\n",
    "                model.fit(x_treino, y_treino)\n",
    "\n",
    "                y_previsto = []\n",
    "                for row in x_teste:\n",
    "                    previsao = model.predict(row.reshape(1, -1))\n",
    "                    y_previsto.append(previsao)\n",
    "                mape = mean_absolute_percentage_error(y_teste, y_previsto)\n",
    "                cvs.append(mape)\n",
    "\n",
    "                del x_treino, y_treino, x_teste, y_teste, y_previsto\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n!!! Repeating objective function: {e}.\")\n",
    "            return self.objective_function(particle_arr, i)\n",
    "\n",
    "        particle.fitness = np.array(cvs).mean()\n",
    "\n",
    "        self.particles.append(particle)\n",
    "\n",
    "        del cvs, model\n",
    "        gc.collect()\n",
    "        return particle.fitness\n",
    "\n",
    "    def iters_dataframe(self):\n",
    "        df = pandas.DataFrame()\n",
    "        for i in range(len(self.iters)):\n",
    "            part = self.iters[i]\n",
    "            df = pandas.concat([df, pandas.DataFrame.from_dict({\n",
    "                \"N_estimators\": part.estimators,\n",
    "                \"Max_depth\": part.max_depth,\n",
    "                \"Booster\": part.booster,\n",
    "                \"Lambda\": part.reg_lambda,\n",
    "                \"Alpha\": part.reg_alpha,\n",
    "                \"Fitness\": part.fitness,\n",
    "                \"Base Seed\": self.seed,\n",
    "            }, orient='index').T], ignore_index=True)\n",
    "        return df\n",
    "\n",
    "    def save_iters_csv(self):\n",
    "        pd_df = self.iters_dataframe()\n",
    "        pd_df.to_csv(f\"parâmetros/PSO-XGB ITERS SEED {self.seed}.csv\", sep=\";\", decimal=\",\", index=True)\n",
    "        del pd_df\n",
    "\n"
   ],
   "id": "eac2c45eafd6ffbf",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Carregar Datasets",
   "id": "72bea874c286c090"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T20:03:54.394756Z",
     "start_time": "2025-01-25T20:03:50.410737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_mesclado = pd.read_csv(\"./dados/dados_mesclados.csv\", sep=';', decimal='.')\n",
    "df_features = pandas.read_csv(\"./dados/fitness_features_agrupado.csv\", sep=\";\", decimal=\".\")\n",
    "\n",
    "df_features = df_features.sort_values(\"MAPE\").head(1).reset_index(drop=True)\n",
    "df_features = pandas.DataFrame(\n",
    "    columns=str(df_features.iloc[0][\"FEATURES\"]).replace(\"(\", '').replace(\")\", '').replace(\"'\", \"\").split(\", \"))\n",
    "\n",
    "dataframes = []\n",
    "for campus, dados in df_mesclado.groupby(\"CAMPUS\"):\n",
    "    dados[\"CAMPUS\"] = campus\n",
    "    for i in range(1, 12 + 1):\n",
    "        lag = dados['CONSUMO'].shift(i)\n",
    "        dados[f'LAG_' + '{:02d}'.format(i)] = lag\n",
    "    dados.dropna(inplace=True)\n",
    "\n",
    "    treino, teste = train_test_split(dados, test_size=12, random_state=SEED, shuffle=False)\n",
    "\n",
    "    dataframes.append(treino)\n",
    "\n",
    "df_consumo = pd.concat(dataframes, ignore_index=True).sort_values(\"CAMPUS\").sort_values(\"DATA\")\n",
    "df_features = df_consumo.drop(df_consumo.drop(df_features, axis=1).columns.to_list(), axis=1).columns\n"
   ],
   "id": "da8936280ebdbac8",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Execução da Otimização"
   ],
   "id": "6c3fa0b86041d608"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-25T20:03:54.416008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "seeds = [10000, 20000, 30000]\n",
    "\n",
    "for seed in seeds:\n",
    "    GAESN(df_consumo, df_features, 15, 20, seed)\n",
    "    GALSTM(df_consumo, df_features, 15, 20, seed)\n",
    "    GARF(df_consumo, df_features, 15, 20, seed)\n",
    "    GAXGB(df_consumo, df_features, 15, 20, seed)\n",
    "    PSOESN(df_consumo, df_features, 15, 20, seed)\n",
    "    PSOLSTM(df_consumo, df_features, 15, 20, seed)\n",
    "    PSORF(df_consumo, df_features, 15, 20, seed)\n",
    "    PSOXGB(df_consumo, df_features, 15, 20, seed)\n"
   ],
   "id": "bfc3cfeda0698ad8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Optimization Results\n",
    "## Fitness Evolution"
   ],
   "id": "19382ac5d858be43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "for model in [\"ESN\", \"LSTM\", \"RF\", \"XGB\"]:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.rcParams['xtick.labelsize'] = 18\n",
    "    plt.rcParams['ytick.labelsize'] = 18\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    plt.rcParams['axes.prop_cycle'] = plt.cycler(\n",
    "        color=[\"blue\", \"red\"])\n",
    "    for optimizer in [\"GA\", \"PSO\"]:\n",
    "        try:\n",
    "            df = pandas.DataFrame()\n",
    "            for seed, name in [(\"10000\", \"A\"), (\"20000\", \"B\"), (\"30000\", \"C\")]:\n",
    "                new_df = pandas.read_csv(f'parâmetros/{optimizer}-{model} ITERS SEED {seed}.csv', sep=\";\", decimal=\",\",\n",
    "                                         header=0)\n",
    "                df[seed] = new_df[\"Fitness\"]\n",
    "            plt.plot(range(1, len(df) + 1), [x for x in df.mean(axis=1)],\n",
    "                     label=f\"{optimizer}-{'XGBoost' if model == 'XGB' else model}\")\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Average Fitness - MAPE')\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor('white')\n",
    "    plt.grid(True, color='grey', linestyle=\"--\", linewidth=0.5)\n",
    "    plt.legend(facecolor='white')\n",
    "    plt.savefig(f\"./resultados/otimização - {model}.png\", bbox_inches='tight')\n",
    "    plt.show()\n"
   ],
   "id": "314c347edf186537",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Best Params",
   "id": "e96e28a1f3ad8603"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best = {}\n",
    "for model in [\"ESN\", \"LSTM\", \"RF\", \"XGB\"]:\n",
    "    df = pandas.DataFrame()\n",
    "    for optimizer in [\"GA\", \"PSO\"]:\n",
    "        for seed in [\"10000\", \"20000\", \"30000\"]:\n",
    "            try:\n",
    "                new_df = pandas.read_csv(f'parâmetros/{optimizer}-{model} ITERS SEED {seed}.csv', sep=\";\", decimal=\",\",\n",
    "                                         header=0)\n",
    "                df = pandas.concat([df, new_df], axis=0)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "    df = df.sort_values(by=[\"Fitness\"])\n",
    "    df[df.isnull()] = None\n",
    "    best[f\"{model}\"] = df[:1]\n",
    "\n"
   ],
   "id": "ef4647ccf7f5aff0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ESN",
   "id": "6c761967e551e852"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "best[\"ESN\"].transpose()",
   "id": "55946f504a5a055a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LSTM",
   "id": "bfaa066a29cd62b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "best[\"LSTM\"].transpose()",
   "id": "70e62db65bfa7bc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### RF",
   "id": "28f5dd650e911d15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "best[\"RF\"].transpose()",
   "id": "576650034834c9c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### XGB",
   "id": "3f65956442f9bc50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "best[\"XGB\"].transpose()",
   "id": "1bc362fa4f3b263e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
